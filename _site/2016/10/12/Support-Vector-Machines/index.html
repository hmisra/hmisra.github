<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Support Vector Machine &middot; entangledDimensions
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hover.css">
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">
  <link rel="stylesheet" href="/public/css/font-awesome.min.css">
  <link href="https://fonts.googleapis.com/css?family=Lora:700|Open+Sans|Roboto" rel="stylesheet">
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">
    <link rel="logo" href="/logo.png">

    <script  type="text/javascript" async src="/public/js/jquery-2.1.4.min.js"></script>
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
    
  </script>
  <!-- RSS -->
  <!--<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">--> 
</head>


  <body class="theme-base-15">
      <progress value="0"></progress>
      <script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '1577054755955354',
      xfbml      : true,
      version    : 'v2.5'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/en_US/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p><br><br>Blog Description</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Blog</a>

    

    
    
      
        
      
    
      
        
            
        
      
    
      
    
      
    
      
        
            
          <a class="sidebar-nav-item" href="/ken/">Ken</a>
      
        
      
    
      
    
<a class="sidebar-nav-item" href="/about">About</a>
<!--    <a class="sidebar-nav-item" href="https://github.com/hmisra/archive/v.zip">Download</a>-->
<!--    <a class="sidebar-nav-item" href="https://github.com/hmisra">GitHub project</a>-->
<!--    <span class="sidebar-nav-item">Currently v</span>-->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2017. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap" style=" position:relative;">
      <div class="masthead" style="background-color: #161b2d;" >
        <div class="container" style="background-color: transparent;" >
           
 <div class="header" style="background-color: transparent;" >  
     
     <table id ="masthead-table" style="background-color: transparent;">
        <tr style="background-color: transparent;">
            <td style="background-color: transparent;">
                <a href="/"><img src="/logo.png" width="35px" length="35px" style="border-radius: 0px; margin-top: 5px;" /></a>
          <h2 class="masthead-title">
            <a href="/" title="Home" style="color: white;">entangledDimensions</a>
            <small></small>
          </h2>
            </td>
            <td style="background-color: transparent;"></td>
            <td style="background-color: transparent;"></td>
            <td style="vertical-align:top;  padding-top:8px; background-color: transparent; "> <h4 class="masthead-title hvr-grow"><a href = "/" style="color: white;">Blog</a></h4></td>
            <td style="vertical-align:top; padding-top:8px; background-color: transparent;"><h4 class="masthead-title  hvr-grow"><a href = "/about/" style="color: white;">About</a></h4></td>
            <!-- <td style="vertical-align:top;  padding-top:8px;"><h4 class="masthead-title hvr-grow"><a href = "/ken/">Ken</a></h4></td> -->
            
        </tr>
    </table>
             </div>
        </div>
      </div>
      <div class="container-content" style="padding-bottom:2%; background-color: #F5f5f5;">
            <script  >$(document).ready(function(){
    
    var getMax = function(){
        return $(document).height() - $(window).height();
    }
    
    var getValue = function(){
        return $(window).scrollTop();
    }
    
    if('max' in document.createElement('progress')){
        // Browser supports progress element
        var progressBar = $('progress');
        
        // Set the Max attr for the first time
        progressBar.attr({ max: getMax() });

        $(document).on('scroll', function(){
            // On scroll only Value attr needs to be calculated
            progressBar.attr({ value: getValue() });
        });
      
        $(window).resize(function(){
            // On resize, both Max/Value attr needs to be calculated
            progressBar.attr({ max: getMax(), value: getValue() });
        });   
    }
    else {
        var progressBar = $('.progress-bar'), 
            max = getMax(), 
            value, width;
        
        var getWidth = function(){
            // Calculate width in percentage
            value = getValue();            
            width = (value/max) * 100;
            width = width + '%';
            return width;
        }
        
        var setWidth = function(){
            progressBar.css({ width: getWidth() });
        }
        
        $(document).on('scroll', setWidth);
        $(window).on('resize', function(){
            // Need to reset the Max attr
            max = getMax();
            setWidth();
        });
    }
});


$(document).ready(function(){
  
  $('#flat').addClass("active");
  $('#progressBar').addClass('flat');
    
  $('#flat').on('click', function(){
    $('#progressBar').removeClass().addClass('flat');
    $('a').removeClass();
    $(this).addClass('active');
    $(this).preventDefault();
  });


  $(document).on('scroll', function(){

      maxAttr = $('#progressBar').attr('max');
      valueAttr = $('#progressBar').attr('value');
      percentage = (valueAttr/maxAttr) * 100;
      
      
  });
  
});</script>
<div style="position:relative; background-image: url(/svm-1.png);  background-size: contain; background-repeat: repeat; width:100%; height:400px;   margin-top: -1em;"><p></p></div>
<div class="container-content-text hvr-glow" style="background-color:white;margin-top: -13rem; border-radius:4px; ">

<div class="posts">
<div class="post" style="padding-left: 60px; padding-top:20px;padding-right:60px; padding-bottom:20px;">
  <h1 class="post-title" >Support Vector Machine</h1>

  <span class="post-date"> &nbsp;<span style="color:#F59B00;">blog</span> published on 12 Oct 2016</span>
    <hr style="margin:1px;">
    <div style="margin:1px;">
    <table>
  <tr>
    <td width= 6em height = 6em style="padding-right:0.78em; padding-top:0.78em; padding-bottom:0.78em;"> <div class="hvr-grow" style="border:2px solid #102366; background-image: url(/himanshu.jpg); background-size: contain;  height:4.7em; width:4.7em; border-radius:2.35em; "></div></td>
    <td style="padding-left:1.2em;  "><div style="margin:0px; padding:0px; height:1rem; color: #102366; font-size: 0.9rem; font-weight: 600; line-height: 1rem;">Himanshu Misra</div> <div style="margin:0px; padding:0px; height:1rem; color: rgba(0, 0, 0, 0.65); font-size: 0.65rem; font-weight: 700; line-height: 1rem; ">Data Scientist @ Legacy.com</div></td>

    <td style="text-align:right;" >
        
        <div style="padding-right:3em;">
            
      <a href="https://www.linkedin.com/in/hmisra"><i class="fa fa-linkedin-square fa-2x hvr-grow " aria-hidden="true" style="color:#102366;"></i></a>
      <a href="https://www.facebook.com/himanshumisra1990"><i class="fa fa-facebook-square fa-2x hvr-grow " aria-hidden="true" style="color:#102366;"></i></a>
        
        <a href="https://twitter.com/himanshumisra"><i class="fa fa-twitter fa-2x hvr-grow " aria-hidden="true" style="color:#102366;"></i></a>
        <a href="https://github.com/hmisra"><i class="fa fa-github fa-2x hvr-grow " aria-hidden="true" style="color:#102366;"></i></a>
            
            </div>
        </td>
  </tr>
</table>
    </div>
<hr>
<div class="roboto" style="padding:0px;">
<h2 id="introduction">Introduction</h2>
<hr />
<p>Support Vector Machines were introduced in 1962 as non-probabilistic binary linear classifiers by Vladimir Vapnik and Alexey Ya. Chervonenkis. Later, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir Vapnik proposed a way to learn non-linear decision boundaries using SVM.</p>

<p>In this post I intend to provide my understanding of SVMâ€™s with as little mathematics as possible.</p>

<h2 id="mathematical-pre-requisite">Mathematical Pre-requisite</h2>
<hr />
<p>I understand that toward the end of previous section I mentioned that there would be very little mathematics, but these mathematical concepts are bare minimum requirements for understanding how does a SVM does what it does.</p>

<ol>
  <li><strong>Dot Product</strong>: $ \vec {u} . \vec{v} $ is an operation which computes projection of vector $ \vec {u} $ on vector $ \vec {v} $ . The result of dot product of 2 vectors is a scalar quantity which can be interpreted as the magnitude of the similarity between 2 vectors with range {-1, 1} where the result of 1 means they point in the same direction, -1 means they point in opposite direction and 0 means they are orthogonal to each other.</li>
</ol>

<h2 id="the-binary-classification-problem">The binary classification problem</h2>
<hr />

<p>The binary classification problem can be defined as assigning one out of 2 classes to n dimensional points. Lets consider an example so that we can visualize it much better.</p>
<center>
<img src="/svm-2.png" width="400" height="400" />
</center>
<p>Here the $ {x_1} $ and $ {x_2} $ are the input dimensions and the label of the data points $ {y} $ is the target dimension. Hence the binary classification problem can be defined as learning a classifier (can also be read as a function) which can predict the label $ {y} $ for unseen data points. SVM falls into a class of classifiers which does so by learning a decision boundary. You can imagine this as a line or hyperplane for higher dimensional data points which tries to separate the 2 classes and the equation of this line or hyperplane can then be used to check which side of the line or hyperplane the new data point falls. In the diagram above you can see that line 1 which is separating the positive and negative classes can be a decision boundary. After learning the parameters of this line once you get a new point you can check which side of the line the point falls into by putting the x into the equation and by checking the sign of the result. Most of the linear classifiers like logistic regression tries to learn the decision boundaries in a similar way.</p>

<p>SVM goes a step further. You can see in the diagram above that there can be multiple lines which can divide the 2 classes. But out of all the possible line one line would be best which would allow classifier to generalize well. For example line 2 might misclassify the positive data points present on the upper left side of the point close to the line 2. This over fitting would restrict the classifier to generalize with new points. To restrict this Logistic Regression uses regularization (restricts the weights of the classifier) but SVM uses a different mathematical solution to find an optimum decision boundary.</p>

<h2 id="support-vector-machine">Support Vector Machine</h2>
<hr />

<p>Lets first intutively try to formalize the notion of optimum decision boundary. Consider the 2 points $ {a} $ and $ {b} $ which falls into negative and positive class respectivily (see the diagram below). What can you notice special about these points ? You can notice that these points are closest to the points of the other class, or to put it in other words you can say that they are the last points in their classes if we consider the dotted lines as our decision boundries. There can be multiple such points which falls close to the dotted decision boundries. Now try to think how can you define an optimal decision boundry with the help of dotted lines. Intutively you can see that the best line you can put would be at the center of the zone between 2 dotted lines.</p>

<center>
<img src="/svm-1.png" width="400" height="400" />
</center>

<p>Now lets formalize this notion mathematically, We can clearly see that the line that we can consider best decision boundary would be at maximum distance from the 2 dotted line. In other words the line would be at maximum distance from $ {a} $ and $  {b} $ . This distance is called the margin. Now we need to find a line which maximizes this margin and that would be our optimal decision boundary. Here we would make an assumption that all the decision boundaries, line 1, 2 and 3 are parallel to each other, the reason for this assumption would be clear very soon.</p>

<p>Lets consider that the optimum line is line 1 which can be defined by the equation (in vector form) :</p>

<script type="math/tex; mode=display">{y}={W}^T {x} + {c}</script>

<p>If we substitute point a and b into this equation we would get:</p>

<script type="math/tex; mode=display">{W}^T {a} + {c} = {1}</script>

<script type="math/tex; mode=display">{W}^T {b} + {c} = {-1}</script>

<p>Now to compute the margin we would need to find the difference between $ {a} $ and $ {b} $ which we can do by subtracting the 2 equations above and we get:</p>

<script type="math/tex; mode=display">{W}^T ({a} - {b}) = 2</script>

<p>Now recall from the linear algebra class that the weight vector $ {W} $ is orthogonal to the line, so to compute the margin we find the difference between $ {a} $ and $ {b} $ in direction of W. For this we would divide both sides with norm of W and it would give us the margin $  {m} $.</p>

<script type="math/tex; mode=display">\dfrac{W^T ({a} - {b})}{||W||} = \dfrac{2}{||W||} = {m}</script>

<blockquote>
  <p>So first lets review what we have done so far and then proceed further, we selected an arbitrary line as our decision boundary assuming that it is our optimal decision boundary. They we took 2 points on the dotted lines which are parallel to the decision boundary we selected. We substituted both of these points in the equation of line and then computed the difference between them. We projected that difference on the unit weight vector which we called our margin.</p>
</blockquote>

<p>Now recall that we decided that the optimum line would be the one which maximises the margin. So we will now create an optimization problem which would give us a weight vector which can maximize this margin.</p>

<p>Also, this would be an constrained optimization problem as we need the $ {W} $ which maximizes the margin but also classifies the points correctly.</p>

<script type="math/tex; mode=display">maximize \dfrac{2}{||W||}</script>

<center> constrained by:  </center>

<script type="math/tex; mode=display">{y_i}({W}^T {x_i} + {c}) \geq 1</script>

<p>To solve this optimization problem we would translate into a form which we can solve. Letsâ€™s first convert this to a equivalent minimization problem, for which we would reciprocate the margin and square the weight:</p>

<script type="math/tex; mode=display">minimize \dfrac{||W||^2}{2}</script>

<center> constrained by:  </center>

<script type="math/tex; mode=display">{y_i}({W}^T {x_i} + {c}) \geq 1</script>

<p>Now this form of optimization problem can be solved using quardatic programming. Here in this post I would not go into Quardatic Programming much but you can take my word for the steps to follow.</p>

<p>Using quardatic programming we can translate the minimization problem mentioned above into standard form</p>

<script type="math/tex; mode=display">F(\alpha) = \sum_{i} \alpha_i - \dfrac{1}{2} \sum_{ij} \alpha_i \alpha_j y_i y_j x_i^T x_j</script>

<center> where </center>

<script type="math/tex; mode=display">\alpha_i \geq \phi</script>

<center>and</center>

<script type="math/tex; mode=display">\sum_{i} \alpha_i y_i = \phi</script>

<p>from by solving these equations we can find the $ \alpha $ which satisfies the conditions. After knowing the value of $ \alpha $ we can retrieve the value of W which minimizes our optimization problem as follows:</p>

<script type="math/tex; mode=display">{W} = \sum_{i} \alpha_i y_i x_i</script>

<p>and having retrieved $ W $ from the equation above we can retirieve the value of {b} from the equations of point $ {a} $ and $ {b} $. Once we have $ W $ and $ b $ we have the equation of decision boundary which have maximum margin and which is an optimal classifier for the classification problem.</p>

<h2 id="summary-and-conclusion">Summary and Conclusion</h2>
<hr />

<p>So lets put in words what we have achieved so far:</p>

<ul>
  <li>We started with taking an arbitrary equation of our optimal decision boundary</li>
</ul>

<script type="math/tex; mode=display">{y}={W}^T {x} + {c}</script>

<ul>
  <li>Then assumed 2 parallel lines to our decision boudary such that they both touch the end points for each class. We called these points $ a $ and $ b $ and got eqation of these points by sustituting in the equation from 1.</li>
</ul>

<script type="math/tex; mode=display">{W}^T {a} + {c} = {1}</script>

<script type="math/tex; mode=display">{W}^T {b} + {c} = {-1}</script>

<ul>
  <li>Then we defined notion of margin in terms of $ W $ the weight vector of our classifier</li>
</ul>

<script type="math/tex; mode=display">\dfrac{2}{||W||} = {m}</script>

<ul>
  <li>We created an optimization problem to find $ W $ that maximize the margin. For this we converted our maximization problem into a quardatic programming problem and converted into a normalized form.</li>
</ul>

<script type="math/tex; mode=display">F(\alpha) = \sum_{i} \alpha_i - \dfrac{1}{2} \sum_{ij} \alpha_i \alpha_j y_i y_j x_i^T x_j</script>

<center> where </center>

<script type="math/tex; mode=display">\alpha_i \geq \phi</script>

<center>and</center>

<script type="math/tex; mode=display">\sum_{i} \alpha_i y_i = \phi</script>

<ul>
  <li>We solve for $ \alpha $ and retrieve W using</li>
</ul>

<script type="math/tex; mode=display">{W} = \sum_{i} \alpha_i y_i x_i</script>

<ul>
  <li>Using $ W $ obtained in step 5 we retrieve $ b $ by substituting $ W $ in equations in step 2.</li>
</ul>

<p>The method described well only if there is a linear decision boundary that can seperate the 2 classes. If the data points of 2 classes are not linearly separable, method described above would provide the decision boundary which can do its best but would not be a perfect decision boundary.</p>


    </div>
    
</div>
    </div>
<div class="share-buttons">
    
<!--    <i class="fa fa-share-square-o fa-2x" style=" color:#102366;" aria-hidden="true"></i>-->
   <center><fieldset class="cat" style="width: 30%;"><legend><b>&nbsp;Share&nbsp;</b> </legend>
    <a href="https://twitter.com/intent/tweet?text=http://hmisra.github.io/2016/10/12/Support-Vector-Machines/" style="padding-right:1em;" title="Share on Twitter"><i class="fa fa-twitter fa-3x hvr-float" style="color:#102366;"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=http://hmisra.github.io/2016/10/12/Support-Vector-Machines/" style="padding-right:1em;" title="Share on Facebook"><i class="fa fa-facebook-square fa-3x hvr-float" style="color:#102366;"></i></a>
    <a href="https://plus.google.com/share?url=http://hmisra.github.io/2016/10/12/Support-Vector-Machines/"  style="padding-right:1em;" title="Share on Google Plus"><i class="fa fa-google-plus fa-3x hvr-float" style="color:#102366;"></i></a>
    <a href="http://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Fhttp://hmisra.github.io/2016/10/12/Support-Vector-Machines/&title=Support Vector Machine&summary=Description of inner workings of SVM&source=http%3A%2F%2Fhttp://hmisra.github.io/2016/10/12/Support-Vector-Machines/" style="padding-right:1em;" title="Share on Linkedin"><i class="fa fa-linkedin-square fa-3x hvr-float" style="color:#102366;"></i></a>
      <a href="https://www.reddit.com/submit?url=http://hmisra.github.io/2016/10/12/Support-Vector-Machines/"  style="padding-right:1em;" title="Share on Reddit"><i class="fa fa-reddit fa-3x hvr-float" style="color:#102366;"></i>
    </a>
    
    
    </fieldset>   
       </center>
    
</div><!-- end share-buttons -->


<div id="disqus_thread" style="padding-left:80px; padding-right:80px; border-bottom:1px solid black;"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'entangleddimensions';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>

<!--
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/10/15/Good-Reads/">
            Good Reads
            <small>15 Oct 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/06/07/Introduction-to-Spark/">
            Introduction to Spark
            <small>07 Jun 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/07/26/Estimating-Truth_Using-Recursive-Bayesian-Filter/">
            Recursive Bayesian Filter
            <small>26 Jul 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
-->
</div>
      </div>
      </div>
        <div class="footer roboto" style=" align:center; vertical-align:middle; min-height:50px;max-height:50px; width:100%; border-top:2px solid #eee; position: relative; bottom:0px; left:0px;padding-top:1%; "><center>
      Powered By <i class="fa fa-github-square fa-lg" aria-hidden="true" style="padding-left:25px; color: #102366;"></i> <i class="fa fa-css3 fa-lg" aria-hidden="true" style="padding-left:25px; "></i><i class="fa fa-html5 fa-lg" aria-hidden="true" style="padding-left:25px;"></i></center>
      </div>
    </div>
    
    <script id="dsq-count-scr" src="//himanshumisra.disqus.com/count.js" async></script>
      <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75408923-1', 'auto');
  ga('send', 'pageview');

</script>
      
  </body>
</html>
